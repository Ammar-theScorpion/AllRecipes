{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greeting</td>\n",
       "      <td>[hi, hello, hey, greetings, good day, good mor...</td>\n",
       "      <td>[Hello!, Hi there!, Welcome! How can I assist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greeting</td>\n",
       "      <td>[how's it going, what's up]</td>\n",
       "      <td>[I'm here to help!, Not much, just here to ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thanks</td>\n",
       "      <td>[Thanks, Thank you, That's helpful, Thanks for...</td>\n",
       "      <td>[Happy to help!, Any time!, My pleasure, You'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>about</td>\n",
       "      <td>[Who are you?, What are you?, Who you are?]</td>\n",
       "      <td>[I'm Xen, your bot assistant, I'm Xen, an Arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name</td>\n",
       "      <td>[what is your name, what should I call you, wh...</td>\n",
       "      <td>[You can call me Xen., I'm Xen!, Just call me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>help</td>\n",
       "      <td>[Could you help me?, give me a hand please, Ca...</td>\n",
       "      <td>[Tell me how can assist you, Tell me your prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>product_info</td>\n",
       "      <td>[tell me about your products, what do you sell]</td>\n",
       "      <td>[We offer a wide range of products. Is there s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>product_info</td>\n",
       "      <td>[can you recommend a product, what's popular]</td>\n",
       "      <td>[Certainly! Our bestsellers include [product 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[Goodbye, See you later, Bye for now, Farewell]</td>\n",
       "      <td>[Goodbye! Have a great day!, See you later! Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[Thanks for your help, Appreciate your assista...</td>\n",
       "      <td>[You're welcome! If you have more questions, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>product_price</td>\n",
       "      <td>[How much does [product_name] cost?, What's th...</td>\n",
       "      <td>[The price for [product_name] is $[product_pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>product_price</td>\n",
       "      <td>[Can you give me the price of [product_name]?,...</td>\n",
       "      <td>[Sure, [product_name] is available for $[produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>product_price</td>\n",
       "      <td>[How expensive is [product_name]?, Is [product...</td>\n",
       "      <td>[[product_name] is reasonably priced at $[prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>product_price</td>\n",
       "      <td>[Tell me more about the pricing of [product_na...</td>\n",
       "      <td>[Certainly! [product_name] comes with a price ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>payment</td>\n",
       "      <td>[What payment methods do you accept?, How can ...</td>\n",
       "      <td>[We accept major credit cards, debit cards, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>order</td>\n",
       "      <td>[Can I cancel my order?]</td>\n",
       "      <td>[Yes]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag                                           patterns  \\\n",
       "0        greeting  [hi, hello, hey, greetings, good day, good mor...   \n",
       "1        greeting                        [how's it going, what's up]   \n",
       "2          thanks  [Thanks, Thank you, That's helpful, Thanks for...   \n",
       "3           about        [Who are you?, What are you?, Who you are?]   \n",
       "4            name  [what is your name, what should I call you, wh...   \n",
       "5            help  [Could you help me?, give me a hand please, Ca...   \n",
       "6    product_info    [tell me about your products, what do you sell]   \n",
       "7    product_info      [can you recommend a product, what's popular]   \n",
       "8         goodbye    [Goodbye, See you later, Bye for now, Farewell]   \n",
       "9         goodbye  [Thanks for your help, Appreciate your assista...   \n",
       "10  product_price  [How much does [product_name] cost?, What's th...   \n",
       "11  product_price  [Can you give me the price of [product_name]?,...   \n",
       "12  product_price  [How expensive is [product_name]?, Is [product...   \n",
       "13  product_price  [Tell me more about the pricing of [product_na...   \n",
       "14        payment  [What payment methods do you accept?, How can ...   \n",
       "15          order                           [Can I cancel my order?]   \n",
       "\n",
       "                                            responses  \n",
       "0   [Hello!, Hi there!, Welcome! How can I assist ...  \n",
       "1   [I'm here to help!, Not much, just here to ass...  \n",
       "2   [Happy to help!, Any time!, My pleasure, You'r...  \n",
       "3   [I'm Xen, your bot assistant, I'm Xen, an Arti...  \n",
       "4   [You can call me Xen., I'm Xen!, Just call me ...  \n",
       "5   [Tell me how can assist you, Tell me your prob...  \n",
       "6   [We offer a wide range of products. Is there s...  \n",
       "7   [Certainly! Our bestsellers include [product 1...  \n",
       "8   [Goodbye! Have a great day!, See you later! Do...  \n",
       "9   [You're welcome! If you have more questions, f...  \n",
       "10  [The price for [product_name] is $[product_pri...  \n",
       "11  [Sure, [product_name] is available for $[produ...  \n",
       "12  [[product_name] is reasonably priced at $[prod...  \n",
       "13  [Certainly! [product_name] comes with a price ...  \n",
       "14  [We accept major credit cards, debit cards, an...  \n",
       "15                                              [Yes]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to combain similar tags on patterns and responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = chat_data['tag']\n",
    "chat_data_combined = []\n",
    "\n",
    "for tag in tags.unique():\n",
    "\n",
    "    rows = chat_data[chat_data['tag'] == tag]\n",
    "    patterns = rows['patterns'].sum()\n",
    "    responses = rows['responses'].sum()\n",
    "\n",
    "    data= {\n",
    "        'tag':tag,\n",
    "        'patterns':patterns,\n",
    "        'responses': responses\n",
    "    }\n",
    "    chat_data_combined.append(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greeting</td>\n",
       "      <td>[hi, hello, hey, greetings, good day, good mor...</td>\n",
       "      <td>[Hello!, Hi there!, Welcome! How can I assist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thanks</td>\n",
       "      <td>[Thanks, Thank you, That's helpful, Thanks for...</td>\n",
       "      <td>[Happy to help!, Any time!, My pleasure, You'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>about</td>\n",
       "      <td>[Who are you?, What are you?, Who you are?]</td>\n",
       "      <td>[I'm Xen, your bot assistant, I'm Xen, an Arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>name</td>\n",
       "      <td>[what is your name, what should I call you, wh...</td>\n",
       "      <td>[You can call me Xen., I'm Xen!, Just call me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help</td>\n",
       "      <td>[Could you help me?, give me a hand please, Ca...</td>\n",
       "      <td>[Tell me how can assist you, Tell me your prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>product_info</td>\n",
       "      <td>[tell me about your products, what do you sell...</td>\n",
       "      <td>[We offer a wide range of products. Is there s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>goodbye</td>\n",
       "      <td>[Goodbye, See you later, Bye for now, Farewell...</td>\n",
       "      <td>[Goodbye! Have a great day!, See you later! Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>product_price</td>\n",
       "      <td>[How much does [product_name] cost?, What's th...</td>\n",
       "      <td>[The price for [product_name] is $[product_pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>payment</td>\n",
       "      <td>[What payment methods do you accept?, How can ...</td>\n",
       "      <td>[We accept major credit cards, debit cards, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>order</td>\n",
       "      <td>[Can I cancel my order?]</td>\n",
       "      <td>[Yes]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag                                           patterns  \\\n",
       "0       greeting  [hi, hello, hey, greetings, good day, good mor...   \n",
       "1         thanks  [Thanks, Thank you, That's helpful, Thanks for...   \n",
       "2          about        [Who are you?, What are you?, Who you are?]   \n",
       "3           name  [what is your name, what should I call you, wh...   \n",
       "4           help  [Could you help me?, give me a hand please, Ca...   \n",
       "5   product_info  [tell me about your products, what do you sell...   \n",
       "6        goodbye  [Goodbye, See you later, Bye for now, Farewell...   \n",
       "7  product_price  [How much does [product_name] cost?, What's th...   \n",
       "8        payment  [What payment methods do you accept?, How can ...   \n",
       "9          order                           [Can I cancel my order?]   \n",
       "\n",
       "                                           responses  \n",
       "0  [Hello!, Hi there!, Welcome! How can I assist ...  \n",
       "1  [Happy to help!, Any time!, My pleasure, You'r...  \n",
       "2  [I'm Xen, your bot assistant, I'm Xen, an Arti...  \n",
       "3  [You can call me Xen., I'm Xen!, Just call me ...  \n",
       "4  [Tell me how can assist you, Tell me your prob...  \n",
       "5  [We offer a wide range of products. Is there s...  \n",
       "6  [Goodbye! Have a great day!, See you later! Do...  \n",
       "7  [The price for [product_name] is $[product_pri...  \n",
       "8  [We accept major credit cards, debit cards, an...  \n",
       "9                                              [Yes]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data_combined = pd.DataFrame(chat_data_combined)\n",
    "chat_data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tag', 'patterns', 'responses'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tag        10 non-null     object\n",
      " 1   patterns   10 non-null     object\n",
      " 2   responses  10 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 368.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "chat_data_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total questions:  48\n",
      "total answers:  38\n"
     ]
    }
   ],
   "source": [
    "count_qu = 0\n",
    "count_an = 0\n",
    "\n",
    "for pattern in chat_data_combined.patterns:\n",
    "    count_qu+=len(pattern)\n",
    "\n",
    "for response in chat_data_combined.responses:\n",
    "    count_an+=len(response)    \n",
    "print('total questions: ',count_qu)\n",
    "print('total answers: ',count_an)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is the mechanism that allows chatbots analyse what users say, extract essential information and respond with appropriate answers.\n",
    "* Tokenization: split an expression into individual words\n",
    "* * need tokeninzation to to compare the expresion with training data\n",
    "\n",
    "* Highlighting Frequent Tokens\n",
    "* * mark repeated tokens within a tag as Frequent\n",
    "\n",
    "* Define Collisions\n",
    "* * those tokens that keep repeating across different conversations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with NLP we need to preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "* tokenization\n",
    "* lowercase\n",
    "* removing any special characters\n",
    "* stemming, lemmatiztion\n",
    "* text normaliztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/ammat/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular') #package includes tokens, stop words ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi hello hey greetings good day good morning good evening how's it going what's up\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need to use nltk lib to help us with cleaning the data\n",
    "import nltk\n",
    "for x in ['patterns', 'responses']:\n",
    "    chat_data_combined['sentences_'+x] = chat_data_combined[x].apply(lambda x:nltk.sent_tokenize(' '.join(x)))\n",
    "chat_data_combined['sentences_patterns'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['patterns', 'responses']:\n",
    "    chat_data_combined['tokens_'+x] = chat_data_combined['sentences_'+x].apply(lambda sentence: nltk.word_tokenize(''.join(sentence).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"hi hello hey greetings good day good morning good evening how's it going what's up\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'hello',\n",
       " 'hey',\n",
       " 'greetings',\n",
       " 'good',\n",
       " 'day',\n",
       " 'good',\n",
       " 'morning',\n",
       " 'good',\n",
       " 'evening',\n",
       " 'how',\n",
       " \"'s\",\n",
       " 'it',\n",
       " 'going',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'up']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chat_data_combined['sentences_patterns'][0])\n",
    "chat_data_combined['tokens_patterns'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word normaliztion\n",
    "### Stemming and Lemmatization\n",
    "* stemming is removing suffix and prefix to match a said rule without considiring sentex, context or grammer to reduce input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "like\n",
      "like\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer as pstammer\n",
    "stammer = pstammer()\n",
    "\n",
    "example = ['like', 'Likes', 'liking']\n",
    "for i in example:\n",
    "    print(stammer.stem(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    return [stammer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data_combined['stem_patterns']=chat_data_combined['tokens_patterns'].apply(lambda x: stemming(x))\n",
    "chat_data_combined['stem_responses']=chat_data_combined['tokens_responses'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'hello',\n",
       " 'hey',\n",
       " 'greet',\n",
       " 'good',\n",
       " 'day',\n",
       " 'good',\n",
       " 'morn',\n",
       " 'good',\n",
       " 'even',\n",
       " 'how',\n",
       " \"'s\",\n",
       " 'it',\n",
       " 'go',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'up']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data_combined['stem_patterns'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here as you see, Stemming reduces the dimensionality and accuracy of the words. We need to try both of the methods, Stemming and Lemmatization, and consider the one we need later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that Stemming is faster while lemmatization is slower but yet more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizating(tokens):\n",
    "    return [lemma.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data_combined['lemma_responses']=chat_data_combined['tokens_responses'].apply(lambda x: lemmatizating(x))\n",
    "chat_data_combined['lemma_patterns']=chat_data_combined['tokens_patterns'].apply(lambda x: lemmatizating(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hello', 'hey', 'greeting', 'good', 'day', 'good', 'morning', 'good', 'evening', 'how', \"'s\", 'it', 'going', 'what', \"'s\", 'up']\n",
      "['hi', 'hello', 'hey', 'greet', 'good', 'day', 'good', 'morn', 'good', 'even', 'how', \"'s\", 'it', 'go', 'what', \"'s\", 'up']\n"
     ]
    }
   ],
   "source": [
    "print(chat_data_combined['lemma_patterns'].iloc[0])\n",
    "print(chat_data_combined['stem_patterns'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for an ecommerce chatbot, we may or may not remove them, depending on how accurate we want it to be\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, let's get stopping words in our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 ['how', 'it', 'what', 'up', 'you', 'that', 'for', 'the', 'who', 'are', 'you', 'what', 'are', 'you', 'who', 'you', 'are', 'what', 'is', 'your', 'what', 'should', 'i', 'you', 'your', 'you', 'me', 'me', 'a', 'can', 'you', 'what', 'can', 'you', 'do', 'for', 'me', 'i', 'a', 'i', 'a', 'me', 'me', 'about', 'your', 'what', 'do', 'you', 'can', 'you', 'a', 'what', 'you', 'for', 'now', 'for', 'your', 'your', 'you', 'and', 'how', 'does', 'what', 'the', 'of', 'me', 'the', 'for', 'you', 'me', 'the', 'of', 'what', 'the', 'of', 'how', 'is', 'is', 'me', 'more', 'about', 'the', 'of', 'what', 'do', 'you', 'how', 'can', 'i', 'can', 'i', 'my']\n"
     ]
    }
   ],
   "source": [
    "stopword = stopwords.words('english')\n",
    "stopword_in_data = []\n",
    "for words in chat_data_combined['tokens_patterns']:\n",
    "    for word in words:\n",
    "        if word in stopword:\n",
    "            stopword_in_data.append(word)\n",
    "print(len(stopword_in_data), stopword_in_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92 stopping words in patterns only! let's keep them for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about punctuations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "* we need to count the number of frequency of words in each sentence by converting them to numerical values\n",
    "* also needed to compare user query with the data we have as explained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
